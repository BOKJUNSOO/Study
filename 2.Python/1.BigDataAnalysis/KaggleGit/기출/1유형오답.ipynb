{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "책 코드 )\n",
    "\n",
    "1) sort_value와 스택참조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28490.598645951555\n",
      "28490.598645951555\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r\"C:\\Users\\brian\\Desktop\\JUNSOO\\study\\2.Python\\1.BigDataAnalysis\\BooK\\data\\boston.csv\")\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "\n",
    "\"\"\"boston 데이터 세트의 MEDV 칼럼을 기준으로 30번째로 큰 값을 1~29번째로 큰 값에 적용한다.\n",
    "    그리고 MEDV 칼럼의 평균값, 중위값, 최솟값, 최댓값 순으로 한줄에 출력하시오.\"\"\"\n",
    "stack = df.sort_values(by = \"MEDV\", ascending = False)\n",
    "stack = stack.reset_index(drop = True)\n",
    "data = stack.loc[29, \"MEDV\"]\n",
    "stack.loc[0:30,\"MEDV\"] = data\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"boston 데이터 세트의 AGE 컬럼을 소수점 첫 번쨰 자리에서 반올림 하고,\n",
    "    가장 많은 비중을 차지하는 AGE 값과 그 개수를 차례대로 출력하시오.\n",
    "    즉, AGE 칼럼의 최빈값과 그 개수를 출력하시오.\"\"\"\n",
    "df[\"AGE\"] = round(df[\"AGE\"],0).astype(int)\n",
    "stack = pd.DataFrame(df.groupby([\"AGE\"])[\"AGE\"].count())\n",
    "stack.columns = [\"AGE_COUNT\"]\n",
    "stack = stack.reset_index()\n",
    "stack = stack.sort_values(by = \"AGE_COUNT\", ascending = False)\n",
    "\n",
    "\n",
    "\"\"\"boston 데이터 세트의 TAX 칼럼을 \n",
    "    오름차순으로 정렬한 결과와 내림차순으로 정렬한 결과를 각각구한다\n",
    "    그리고 각 순번에 맞는 오름차순값과 내림차순 값의차이를 구하여 분산값을 출력하시오.\"\"\"\n",
    "\n",
    "temp1 = df.sort_values(by = \"TAX\", ascending= True).reset_index(drop=True)\n",
    "temp2 = df.sort_values(by = \"TAX\", ascending= False).reset_index(drop=True)\n",
    "stack = pd.concat([temp1[\"TAX\"],temp2[\"TAX\"]], axis = 1)\n",
    "stack.columns = [\"tax1\",\"tax2\"]\n",
    "stack[\"diff\"] = abs(stack[\"tax2\"] - stack[\"tax1\"])\n",
    "val = stack[\"diff\"].var()\n",
    "print(val)\n",
    "\n",
    "\n",
    "new_data1 = df.sort_values(by = \"TAX\", ascending=False) # 내림차순\n",
    "new_data2 = df.sort_values(by = \"TAX\", ascending=True) # 오름차순\n",
    "\n",
    "new_data1 = new_data1[\"TAX\"].reset_index(drop=True) # 인덱스 설정을 해야 concat시 새로운 인덱스를 기준으로 병합\n",
    "new_data2 = new_data2[\"TAX\"].reset_index(drop=True)\n",
    "\n",
    "data = pd.concat([new_data1, new_data2], axis = 1)  # concat시 series 데이터는 자동으로 dataframe으로 변환?\n",
    "data[\"차이\"] = abs(data.iloc[:,0] - data.iloc[:,1])\n",
    "print(data[\"차이\"].var())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1번 소스코드)\n",
    "\n",
    "1) df.columns = [\"col1_name\",\"col2_name\"]\n",
    "    \n",
    "    integer location\n",
    "\n",
    "2) value_count()\n",
    "\n",
    "3) **중요**\n",
    "    - reset_index()를 사용가능한 경우 - grouping series 에서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     channelId  channelTitle\n",
      "0     UC-0229MiVYnQoUxdLJnFl_g             1\n",
      "1     UC-0C8yVGJy-cS4FGlYKelWw             1\n",
      "2     UC-0tICWyA0_AJAVMBHL8GdQ             1\n",
      "3     UC-2Y8dQb0S6DtpxNgAKoJKA             1\n",
      "4     UC-5acDjU6XFWu-__G17V_YQ             1\n",
      "...                        ...           ...\n",
      "1765  UCznImSIaxZR7fdLCICLdgaQ             1\n",
      "1766  UCzt24ffdUiJvJAm5HY084aw             1\n",
      "1767  UCzteSXznVjwESizsCcSKSWw             1\n",
      "1768  UCzxXBQnJy9guqVNM6KXi0Ig             1\n",
      "1769  UCzz58-H0wzmGoFi3MO_9ePQ             1\n",
      "\n",
      "[1770 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/Datamanim/datarepo/main/youtube/youtube.csv\",index_col=0)\n",
    "stack = df[[\"channelTitle\",\"categoryId\"]]\n",
    "# stack.columns = [[\"what\",\"the\"]]\n",
    "\n",
    "\"\"\"채널명을 바꾼 케이스가 있는지 확인하고 싶다. \n",
    "channelId의 경우 고유값이므로 이를 통해 채널명을 한번이라도 바꾼 채널의 갯수를 구하여라\"\"\"\n",
    "\n",
    "# 1)\n",
    "change = df[['channelTitle','channelId']].drop_duplicates()\n",
    "# groupby 된 series 데이터를 dataframe으로 변경/ reset_index 바로 사용 불가능\n",
    "change = pd.DataFrame(change.groupby(\"channelId\")[\"channelId\"].count())\n",
    "# 해당 정수위치의 컬럼명을 지정\n",
    "change.columns = [\"name_count_per_id\"]\n",
    "# dataframe으로 변경된 자료는 reset index를 사용할 수 있다.\n",
    "change = change.reset_index()\n",
    "\n",
    "# 2)\n",
    "change = df[['channelTitle','channelId']].drop_duplicates()\n",
    "# grouping을 channelId로 하고channelTitle의 갯수를 카운팅하면 reset_index 바로 사용 가능\n",
    "change = change.groupby(\"channelId\")[\"channelTitle\"].count().reset_index()\n",
    "\n",
    "# 3) grouping을 사용하지 않고 갯수 카운팅\n",
    "change = df[['channelTitle','channelId']].drop_duplicates().channelId.value_counts()\n",
    "target = change[change>1]\n",
    "#print(len(target))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2번 소스코드)\n",
    "\n",
    "1) .sort_values().drop_duplicates(\"column\", keep = \"last or first\")\n",
    "    - 최근에 갱신된 데이터 or 처음에 작성된 데이터를 추출하기에 유용하다.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "channel =pd.read_csv('https://raw.githubusercontent.com/Datamanim/datarepo/main/youtube/channelInfo.csv')\n",
    "video =pd.read_csv('https://raw.githubusercontent.com/Datamanim/datarepo/main/youtube/videoInfo.csv')\n",
    "\n",
    "\"각 데이터의 ‘ct’컬럼을 시간으로 인식할수 있게 datatype을 변경하고 video 데이터의 videoname의 각 value 마다 몇개의 데이터씩 가지고 있는지 확인하라\"\n",
    "channel.ct = pd.to_datetime(channel.ct)\n",
    "video.ct = pd.to_datetime(video.ct)\n",
    "\n",
    "\"Channel 데이터중 2021-10-03일 이후 각 채널의 처음 기록 됐던 구독자 수(subcnt)를 출력하라\"\n",
    "# groupby 쓰지말고 하는 연습하기\n",
    "answer = channel.loc[channel.ct >= pd.to_datetime(\"2021-10-03\")] # datetime 데이터 타입은 형식 그냥 작성하면 알아서 계산해줌..\n",
    "answer = answer.sort_values(by = \"ct\", ascending = True).drop_duplicates(\"channelname\", keep = \"first\")[[\"channelname\",\"subcnt\"]]\n",
    "\n",
    "\"\"\"공범 컨텐츠의 경우 19:00시에 공개 되는것으로 알려져있다.\n",
    " 공개된 날의 21시의 viewcnt, ct, videoname 으로 구성된 데이터 프레임을 viewcnt를 내림차순으로 정렬하여 출력하라\"\"\"\n",
    "\n",
    "target = video.copy()\n",
    "target[\"hour\"] = target.ct.dt.hour\n",
    "\n",
    "target = target.loc[target.hour == 21]\n",
    "answer = target.sort_values(by = \"ct\", ascending = True).drop_duplicates(\"videoname\",keep=\"first\").reset_index(drop = True)\n",
    "answer = answer[[\"viewcnt\",\"ct\",\"videoname\"]]\n",
    "\n",
    "\"각채널의 2021-10-03 03:00:00 ~ 2021-11-01 15:00:00 까지 구독자수 (subcnt) 의 증가량을 구하여라***\"\n",
    "start = channel.loc[channel.ct.dt.strftime(\"%Y-%m-%d %H\") == '2021-10-03 03']\n",
    "end = channel.loc[channel[\"ct\"].dt.strftime(\"%Y-%m-%d %H\") == '2021-11-01 15']\n",
    "\n",
    "# 필요한 컬럼만 선택\n",
    "# concat을 위한 reset_index\n",
    "start_stack = start[[\"channelname\",\"subcnt\"]].reset_index(drop=True)\n",
    "end_stack = end[[\"channelname\",'subcnt']].reset_index(drop=True)\n",
    "\n",
    "# 순서대로 컬럼명을 재 설정 (컬럼 수 맞추기)\n",
    "# 원래 인덱스 정보는 삭제했으므로 컬럼을 두개만 이용 \n",
    "start_stack.columns = [\"channelname\",\"start_sub\"]\n",
    "end_stack.columns = [\"channelname\",\"end_sub\"]\n",
    "\n",
    "#stack = pd.concat([start_stack, end_stack],axis=1)\n",
    "stack = pd.merge(start_stack, end_stack)    # 중복되는 컬럼은 생략하고 병합\n",
    "stack[\"del\"] = stack[\"end_sub\"] - stack[\"start_sub\"]\n",
    "\n",
    "#print(stack[[\"channelname\",\"del\"]].head())  # concat으로 데이터 합치면 모든 channelname 을 출력\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3번 소스코드)\n",
    "\n",
    "1) drop() : columns 를 삭제 or index array 를 파라미터로 작성 가능\n",
    "\n",
    "2) isin() : Series.isin(values) or df.isin(values) \n",
    "    \n",
    "    확인하고자 하는 values의 값이 해당 series or df에 있는지를 확인\n",
    "\n",
    "3) contains() : str.contains(\"string\") : string의 포함 여부에 따라 bool 을 리턴\n",
    "\n",
    "4) 그외 : \n",
    "    1) groupby 한 데이터는 series를 리턴한다. reset_index(drop = False) 이용\n",
    "    2) 시리즈 데이터의 sort_values메서드는 by parameter을 필요로하지 않는다\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df= pd.read_csv('https://raw.githubusercontent.com/Datamanim/datarepo/main/worldcup/worldcupgoals.csv')\n",
    "\n",
    "\n",
    "\"\"\"년도 표기가 4자리 숫자로 구성 안된 케이스를 제외한 데이터프레임을 df2라고 정의하고\n",
    " 데이터의 행의 숫자를 출력하라\"\"\"\n",
    "df[\"year\"] = df[\"Years\"].str.split(\"-\")\n",
    "\n",
    "# apply 메서드로 조건을 만족하는 인덱스 arr 를 찾는다\n",
    "condition = df[\"year\"].apply(lambda x: \"0\" in x)\n",
    "arr = df[condition].index\n",
    "arr = list(arr)\n",
    "\n",
    "# drop으로 해당 row를 삭제\n",
    "df2 = df.drop(arr)\n",
    "\n",
    "# isin (is in !) loc 으로 인덱스를 비교하며 선택\n",
    "df2 = df.loc[~df.index.isin(arr)] # check isin 메서드\n",
    "\n",
    "def func(x):\n",
    "    for value in x:\n",
    "        if len(str(value)) != 4:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# df[\"check\"] = df[\"year\"].apply(func)\n",
    "\n",
    "\n",
    "\"\"\"월드컵 출전횟수를 나타내는 ‘LenCup’ 컬럼을 추가하고 4회 출전한 선수의 숫자를 구하여라\"\"\"\n",
    "\n",
    "\n",
    "# 시리즈내의 리스트로 저장된 원소의 길이\n",
    "df[\"LenCup\"] = df[\"year\"].str.len() # str.len\n",
    "stack = df.loc[df[\"LenCup\"] ==4]\n",
    "# stack.shape[0]\n",
    "\n",
    "\"\"\"이름에 ‘carlos’ 단어가 들어가는 선수의 숫자는 몇 명인가? (대, 소문자 구분 x)\"\"\"\n",
    "# contain 메서드\n",
    "result = len(df[df.Player.str.lower().str.contains('carlos')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3회)\n",
    "\n",
    "(시리즈 자료는 list or set으로 변환이 가능하다)\n",
    "\n",
    "1990년도는 해당년도 평균 이하 GDP를 가지고, 2010년도에는 해당년도 평균 이상 GDP를 가지는 국가의 숫자를 구하여라\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df =pd.read_csv('https://raw.githubusercontent.com/Datamanim/datarepo/main/krdatacertificate/e3_p1_2.csv')\n",
    "stack1 = df.loc[df[\"Year\"] == 1990]\n",
    "stack2 = df.loc[df[\"Year\"] == 2010]\n",
    "\n",
    "mean1 = stack1[\"Value\"].mean()\n",
    "mean2 = stack2[\"Value\"].mean()\n",
    "\n",
    "data1 = set(stack1.loc[stack1[\"Value\"] <= mean1][\"Country Name\"].unique())\n",
    "data2 = set(stack2.loc[stack2[\"Value\"] >= mean2][\"Country Name\"].unique())\n",
    "result = len(data1.intersection(data2))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4회)\n",
    "\n",
    "(.str) --> 시리즈내의 작성된 데이터가 문자열일경우 유용하다 !!!\n",
    "\n",
    "Temperature컬럼에서  \n",
    "\n",
    "숫자가 아닌 문자들을 제거후 \n",
    "\n",
    "숫자 타입으로 바꾸고 \n",
    "\n",
    "3분위수에서 1분위수의 차이를 소숫점 이하 2자리까지 구하여라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Datamanim/datarepo/main/krdatacertificate/e4_p1_1.csv')\n",
    "\n",
    "# .str 은 시리즈데이터 내 원소들이 string type 일때 적용하는 메서드\n",
    "# python 의 string type의 메서드와 거의 일치 !!!\n",
    "df[\"Temperature\"] = df[\"Temperature\"].str.replace(\"*\",\"\")\n",
    "df[\"Temperature\"] = df[\"Temperature\"].astype(float)\n",
    "stack = df[[\"Temperature\"]].describe().T\n",
    "\n",
    "result = round(stack[\"75%\"] - stack[\"25%\"], 2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4회)\n",
    "\n",
    "(.str.split() and .str.get(index))\n",
    "\n",
    "date_added가 2018년 1월 이면서 \n",
    "\n",
    "country가 United Kingdom 단독 제작인 데이터의 갯수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Datamanim/datarepo/main/krdatacertificate/e4_p1_3.csv')\n",
    "df[\"new_date\"] = df[\"date_added\"].str.split(\",\")\n",
    "\n",
    "df[\"year\"] = df[\"new_date\"].str.get(1)\n",
    "df[\"month_day\"] = df[\"new_date\"].str.get(0)\n",
    "df.drop(columns = [\"new_date\",\"date_added\"], inplace = True)\n",
    "\n",
    "df[\"month_day\"] = df[\"month_day\"].str.split(\" \")\n",
    "df[\"month\"] = df[\"month_day\"].str.get(0)\n",
    "df[\"day\"] = df[\"month_day\"].str.get(1)\n",
    "df.drop(columns=\"month_day\" , inplace = True)\n",
    "\n",
    "condition = (df[\"year\"] == \" 2018\")&(df[\"month\"] == \"January\")&(df[\"country\"] == \"United Kingdom\")\n",
    "\n",
    "print(df.loc[condition].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5회)\n",
    "\n",
    "1) loc : 정수위치가 아닌 인덱스를 파라미터로 받는다. and 시리즈 데이터를 만족하는 row !!\n",
    "\n",
    "2) idxmax() : 그룹핑이후(시리즈데이터) 집계할때 인덱스를 반환한다.\n",
    "\n",
    "3) 그룹핑 : 결과는 시리즈 데이터이고 (시리즈 데이터는) 컨디션으로 받을 수 있다.\n",
    "\n",
    "초중고 도내,도외 전입인원에서 초중고 도내, 도외 전출인원을 뺀값이다.\n",
    "\n",
    " 각년도별로 가장 큰 순유입인원을 가진 지역구의 순유입인원을 구하고 \n",
    " \n",
    " 전체 기간의 해당 순유입인원들의 합을 구하여라\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Datamanim/datarepo/main/krdatacertificate/e5_p1_3.csv')\n",
    "df.head(5)\n",
    "# 1)\n",
    "df['순유입인원'] = \\\n",
    "df[[x for x in df.columns if '전입' in x]].sum(axis=1) -\\\n",
    "df[[x for x in df.columns if '전출' in x ]].sum(axis=1)\n",
    "\n",
    "result = df.groupby('년도')['순유입인원'].max().sum().sum()\n",
    "#print(result)\n",
    "\n",
    "# 2) idxmax() method\n",
    "df[\"전입인원\"] = df[\"초등학교_전입_도내\"] + df[\"초등학교_전입_도외\"] \\\n",
    "                +df[\"중학교_전입_도내\"] + df[\"중학교_전입_도외\"] \\\n",
    "                +df[\"고등학교_전입_도내\"] + df[\"고등학교_전입_도외\"]\n",
    "\n",
    "df[\"전출인원\"] = df[\"초등학교_전출_도내\"] + df[\"초등학교_전출_도외\"] \\\n",
    "                +df[\"중학교_전출_도내\"] + df[\"중학교_전출_도외\"] \\\n",
    "                +df[\"고등학교_전출_도내\"] + df[\"고등학교_전출_도외\"]\n",
    "\n",
    "df[\"순유입인원\"] = df[\"전입인원\"] - df[\"전출인원\"]\n",
    "stack = df[[\"지역\",\"년도\",\"순유입인원\"]]\n",
    "# 인덱스 필터링 (gropuby는 series 데이터를 반환)\n",
    "max_ = stack.groupby([\"년도\"])[\"순유입인원\"].idxmax() # 인덱스를 반환\n",
    "print(max_)\n",
    "\n",
    "max_reigion = stack.loc[max_]\n",
    "print(max_reigion)\n",
    "\n",
    "result = max_reigion[\"순유입인원\"].sum()\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6회)\n",
    "\n",
    "(타임시리즈, string 메서드 공부하기..)\n",
    "\n",
    "각 구급 보고서 별 출동시각과 신고시각의 차이를 \n",
    "\n",
    "‘소요시간’ 컬럼을 만들고\n",
    "\n",
    "초(sec)단위로 구하고 \n",
    "\n",
    "소방서명 별 소요시간의 평균을 오름차순으로 정렬 했을때 3번째로 작은 소요시간의 값과 소방서명을 출력하라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Datamanim/datarepo/main/krdatacertificate/e6_p1_1.csv')\n",
    "\n",
    "#print(df.head())\n",
    "#df.info()\n",
    "df['소요시각'] = (\n",
    "                pd.to_datetime(\n",
    "                    df['출동일자'].astype('str') + df['출동시각'].astype('str').str.zfill(6)\n",
    "                )\n",
    "\n",
    "                - \n",
    "    \n",
    "                pd.to_datetime(\n",
    "                    df['신고일자'].astype('str') + df['신고시각'].astype('str').str.zfill(6)\n",
    "                )\n",
    "                ).dt.total_seconds()\n",
    "\n",
    "\n",
    "result = df.groupby(['소방서명'])['소요시각'].mean().sort_values().reset_index().iloc[2].values\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7회)\n",
    "\n",
    "(.quantile() 다시보기)\n",
    "\n",
    "var_6 컬럼의 1,3사분위수 각각 IQR의 1.5배 벗어난 이상치의 숫자를 구하라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/Datamanim/datarepo/main/krdatacertificate/e7_p3.csv')\n",
    "\n",
    "#1) 첫번째 방법\n",
    "\n",
    "q1 = df[\"var_6\"].quantile(0.25) # 해당 메서드는 특정 셀을 포인팅! float return\n",
    "q3 = df[\"var_6\"].quantile(0.75)\n",
    "iqr = q3-q1\n",
    "#print(iqr)\n",
    "\n",
    "max_= q3 + 1.5 * iqr\n",
    "min_ = q1 - 1.5 * iqr\n",
    "\n",
    "r = df.loc[(df[\"var_6\"] < min_)|(df[\"var_6\"]>max_)].shape[0]\n",
    "print(r)\n",
    "\n",
    "\n",
    "#2) 두번째 방법\n",
    "stack = df[[\"var_6\"]]\n",
    "dis = stack.describe().T\n",
    "IQR = dis[\"75%\"] - dis[\"25%\"]\n",
    "\n",
    "max_out = dis[\"75%\"] + 1.5 * IQR    # series\n",
    "min_out = dis[\"25%\"] - 1.5 * IQR\n",
    "\n",
    "max__ = max_out.iloc[0] # 특정셀 포인팅\n",
    "min__ = min_out.iloc[0]\n",
    "print(stack.loc[stack[\"var_6\"] >= max__].count())\n",
    "print(stack.loc[stack[\"var_6\"] <= min__].count())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
