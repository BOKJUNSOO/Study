{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark api with pyspark.sql SparkSession\n",
    "- 1) .filter() = .where()\\\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.filter.html\n",
    "- 2) .select()\\\n",
    "https://sparkbyexamples.com/pyspark/select-columns-from-pyspark-dataframe/\n",
    "- 3) .isNotNull()\n",
    "- 4) .pivot()\\\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.GroupedData.pivot.html\n",
    "- 5) .collect()\\\n",
    "https://sparkbyexamples.com/pyspark/pyspark-collect/\n",
    "- 6) .sort()\n",
    "- 7) .orderBy()\\\n",
    "https://sparkbyexamples.com/pyspark/pyspark-orderby-and-sort-explained/\n",
    "- 8) .groupBy()\n",
    "- 9) .crossJoin()   ..?\n",
    "- 10) .agg()\\\n",
    "https://sparkbyexamples.com/pyspark/pyspark-groupby-agg-aggregate-explained/\n",
    "- 11) .alias()\n",
    "- 12) .toDF()\\\n",
    "https://sparkbyexamples.com/pyspark/pyspark-todf-with-examples/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) DataFrame.filter()\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.filter.html\n",
    "\n",
    "- parameter = condition(BooleanType)\n",
    "\n",
    "    condition 형태는 df.col == something or \"col1 == something\"\n",
    "- return = DataFrame\n",
    "\n",
    "- 컨디션을 만족하는 Row의 데이터들로 dataframe을 만든다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
    "\n",
    "df.filter(df.age > 3).show()\n",
    "#+---+----+\n",
    "#|age|name|\n",
    "#+---+----+\n",
    "#|  5| Bob|\n",
    "#+---+----+\n",
    "df.where(df.age == 2).show()\n",
    "#+---+-----+\n",
    "#|age| name|\n",
    "#+---+-----+\n",
    "#|  2|Alice|\n",
    "#+---+-----+\n",
    "\n",
    "df.filter(\"age > 3\").show()\n",
    "#+---+----+\n",
    "#|age|name|\n",
    "#+---+----+\n",
    "#|  5| Bob|\n",
    "#+---+----+\n",
    "df.where(\"age = 2\").show()\n",
    "#+---+-----+\n",
    "#|age| name|\n",
    "#+---+-----+\n",
    "#|  2|Alice|\n",
    "#+---+-----+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) DataFrame.select()\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.filter.html\n",
    "\n",
    "- parameter = cols(str,column or list)\n",
    "\n",
    "NOTE) 컬럼선택\n",
    "\n",
    "df.col1 , \"col1\" , F.col(col1)\n",
    "\n",
    "- return = DataFrame\n",
    "\n",
    "- select의 파라미터로 들어간 컬럼으로 데이터프레임을 구성한다.\n",
    "- nested한 컬럼내의 데이터도 뽑아낼 수 있다..!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#+----------------------+-----+------+\n",
    "#|name                  |state|gender|\n",
    "#+----------------------+-----+------+\n",
    "#|[James,, Smith]       |OH   |M     |\n",
    "#|[Anna, Rose, ]        |NY   |F     |\n",
    "#|[Julia, , Williams]   |OH   |F     |\n",
    "#|[Maria, Anne, Jones]  |NY   |M     |\n",
    "#|[Jen, Mary, Brown]    |NY   |M     |\n",
    "#|[Mike, Mary, Williams]|OH   |M     |\n",
    "#+----------------------+-----+------+\n",
    "\n",
    "df2.select(\"name\").show(truncate=False)\n",
    "\"\"\"\n",
    "+----------------------+\n",
    "|name                  |\n",
    "+----------------------+\n",
    "|[James,, Smith]       |\n",
    "|[Anna, Rose, ]        |\n",
    "|[Julia, , Williams]   |\n",
    "|[Maria, Anne, Jones]  |\n",
    "|[Jen, Mary, Brown]    |\n",
    "|[Mike, Mary, Williams]|\n",
    "+----------------------+\n",
    "\"\"\"\n",
    "df2.select(\"name.*\").show(truncate=False)\n",
    "'''\n",
    "+---------+----------+--------+\n",
    "|firstname|middlename|lastname|\n",
    "+---------+----------+--------+\n",
    "|James    |null      |Smith   |\n",
    "|Anna     |Rose      |        |\n",
    "|Julia    |          |Williams|\n",
    "|Maria    |Anne      |Jones   |\n",
    "|Jen      |Mary      |Brown   |\n",
    "|Mike     |Mary      |Williams|\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) column.isNotNull()\n",
    "\n",
    "- return = condition\n",
    "- filter 의 parameter인 condition 으로 사용가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) GroupedData.pivot()\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.GroupedData.pivot.html\n",
    "\n",
    "- parameter \\\n",
    " => pivot_col : str or values : optional['list']\n",
    "\n",
    "- return = GroupedData\n",
    "\n",
    "- Grouping된 데이터에서 pivot이 되는 컬럼내의 데이터가 새로운 컬럼으로 데이터 프레임이 리턴된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다음과 같은 데이터프레임에서 product별 contry의 생산량을 보는 니즈가 있다해보자.\n",
    "df\n",
    "'''\n",
    "+-------+------+-------+\n",
    "|Product|Amount|Country|\n",
    "+-------+------+-------+\n",
    "|Banana |1000  |USA    |\n",
    "|Carrots|1500  |USA    |\n",
    "|Beans  |1600  |USA    |\n",
    "|Orange |2000  |USA    |\n",
    "|Orange |2000  |USA    |\n",
    "|Banana |400   |China  |\n",
    "|Carrots|1200  |China  |\n",
    "|Beans  |1500  |China  |\n",
    "|Orange |4000  |China  |\n",
    "|Banana |2000  |Canada |\n",
    "|Carrots|2000  |Canada |\n",
    "|Beans  |2000  |Mexico |\n",
    "+-------+------+-------+\n",
    "'''\n",
    "\n",
    "#pivot의 value parameter은 피벗컬럼내의 특정값만 조사할 수 있다(list 형태로)\n",
    "df = df.groupBy('Country').pivot('Product').sum('Amount') \n",
    "df.printSchema()\n",
    "df.show(truncate = False)\n",
    "\n",
    "'''\n",
    "root\n",
    " |-- Product: string (nullable = true)\n",
    " |-- Canada: long (nullable = true)\n",
    " |-- China: long (nullable = true)\n",
    " |-- Mexico: long (nullable = true)\n",
    " |-- USA: long (nullable = true)\n",
    "'''\n",
    "\n",
    "#grouping 하는 기준컬럼이 어디오고, pivot 컬럼내의 값이 어디에 위치하는지 확인하자\n",
    "'''\n",
    "+-------+------+-----+------+----+\n",
    "|Product|Canada|China|Mexico|USA |  \n",
    "+-------+------+-----+------+----+\n",
    "|Orange |null  |4000 |null  |4000|\n",
    "|Beans  |null  |1500 |2000  |1600|\n",
    "|Banana |2000  |400  |null  |1000|\n",
    "|Carrots|2000  |1200 |null  |1500|\n",
    "+-------+------+-----+------+----+ \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) df.collet()\n",
    "- return : row list data\n",
    "- dataframe 의 인스턴스를 뽑아낼 수 있다.\n",
    "- df.select(df.col1).collet() select와 함께 사용하면 특정 컬럼의 값을 뽑아볼 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# advance?\n",
    "\n",
    "[Row(dept_name='Finance', dept_id=10), \n",
    "Row(dept_name='Marketing', dept_id=20), \n",
    "Row(dept_name='Sales', dept_id=30), \n",
    "Row(dept_name='IT', dept_id=40)]\n",
    "\n",
    "for row in dataCollect:\n",
    "    print(row['dept_name'] + \",\" +str(row['dept_id']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6,7) DataFrame.sort() and orderBy\n",
    "\n",
    "- parameter = cols option!\n",
    "- other parameter = ascending \n",
    "- return = dataframe\n",
    "- col에 특정 condition에 의해 분류된 새로운 데이터프레임을 리턴한다.\n",
    "- F.asc() , F.desc() \n",
    "- 내림차순, 오름차순을 위해 이용하자!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 서로다른 컬럼에 서로다른 condition 주기\n",
    "df.sort(\"column1\", \"column2\", ascending=[True, False]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) DataFrame.groupBy()\n",
    "\n",
    "- parameter = col\n",
    "- pandas의 groupby와 동일!\n",
    "- sum() or count() 변수에 따라 어떤것을 같이 쓸지 생각하기\n",
    "- ex) 사용된 프로그래밍 언어의 빈도수 (빈도가 중요하므로 count())\n",
    "- ex2) pushevent를 진행한 횟수 (총 진행한 합이 중요하므로 sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('col1').sum('sumation_col').show(10 , False)\n",
    "# or count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9) DataFrame.crossJoin()\n",
    "- parameter : dataframe (agg한 dataframe?)\n",
    "- return : cartesian(직교의?) product with other DataFrame\n",
    "- 기존 데이터프레임 오른쪽에 붙힌다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10) DataFrame.agg(*exprs)\n",
    " - aggregate : 총계 (어떤 합이아니고 컬럼별 연산후 총계된 데이터를 뽑아낸다로 이해할것)\n",
    " - groupby된 dataframe의 key(grouping된 데이터)를 기준으로 sum, count, avg한 것들을 한번에 표현하기 위해 사용됨.( 여기서 sum, count, avg 는 sql.function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "# groupby multiple columns & agg\n",
    "df.groupBy(\"department\",\"state\") \\\n",
    "    .agg(count(\"*\").alias(\"count\")) \\\n",
    "    .show(truncate=False)\n",
    "\n",
    "from pyspark.sql.functions import sum,avg,max\n",
    "\n",
    "df.groupBy(\"department\") \\\n",
    "    .agg(sum(\"salary\").alias(\"sum_salary\"), \\\n",
    "         avg(\"salary\").alias(\"avg_salary\"), \\\n",
    "         sum(\"bonus\").alias(\"sum_bonus\"), \\\n",
    "         max(\"bonus\").alias(\"max_bonus\") \\\n",
    "     ) \\\n",
    "    .show(truncate=False)\n",
    "\n",
    "import pyspark.sql.fuctions as F\n",
    "\n",
    "stat_df = df.agg(F.countDistinct('user_name').alias('d_user_count'))\n",
    "stat_df = stat_df.crossJoin(df.agg(F.countDistinct('repository_id').alias('d_repo_count')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11) column.alias()\n",
    "- parameter = alias : str \n",
    "- alias('일명') \n",
    "- 컬럼이름 변경하기!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12) DataFrame.toDF()\n",
    "- parameter = *cols : tuple\n",
    "- Returns = DataFrame\n",
    "- 전체 데이터프레임의 컬럼내임을 바꾸고 싶을때!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax\n",
    "toDF()\n",
    "toDF(cols)\n",
    "toDF(schema)\n",
    "toDF(schema, sampleRatio)\n",
    "\n",
    "\n",
    "# Imports\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "            .appName('SparkByExamples.com') \\\n",
    "            .getOrCreate()\n",
    "            \n",
    "# Create RDD           \n",
    "dept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]\n",
    "rdd = spark.sparkContext.parallelize(dept)\n",
    "\n",
    "# Create DataFrame from RDD\n",
    "df = rdd.toDF()\n",
    "df.printSchema()\n",
    "df.show(truncate=False)     #defalut로 _1 , _2 컬럼을 생성한다\n",
    "\n",
    "\n",
    "cols = [\"dept_name\",\"dep_id\"]\n",
    "dfFromRDD1 = rdd.toDF(cols)     # column의 name 설정 가능!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
