{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 정제 withcolumn과 같이 이용하면 좋은 pyspark.sql.fuction\n",
    "\n",
    "- spark 도큐먼트 : https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.col.html\n",
    "1. F.from_json\n",
    "2. F.udf\n",
    "3. regexp_replace (or regexp_extract)\n",
    "4. when, split 절\n",
    "5. F.to_timestamp and trim\n",
    "\n",
    "번외)\n",
    "regexp_replace\n",
    ".agg(F.countDistinct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# withcolumn method\n",
    " - withcolumn을 이용하여 \"기존 컬럼\"의 업데이트, 타입변경, 신규컬럼 값 추가\n",
    " - withcoumn(\"new_col\", \"col\")\n",
    " - 기존컬럼을 기반으로 신규값 생성 or 업데이트 값 생성시 col(\"컬럼명\")을 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 기본적인 쓰임\n",
    " - dataframe.withColumn('추가할 컬럼이름', 연산(col('기존컬럼'))).select(*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
    "#df.withColumn('age2', col('age') + 2).show()\n",
    "#+---+-----+----+\n",
    "#|age| name|age2|\n",
    "#+---+-----+----+\n",
    "#|  2|Alice|   4|\n",
    "#|  5|  Bob|   7|\n",
    "#+---+-----+----+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.F.from_json\n",
    "- nested한 컬럼내의 json 을 추출해낼 수 있다.\n",
    "- parameter = (col , schema, option)\n",
    "- return 값으로 해당 컬럼의 schema를 따라서 json을 추출해낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  기존컬럼을 기반으로 신규값 생성\n",
    "#  업데이트 값 생성시 col(\"컬럼명\")을 이용 with from_json\n",
    "\n",
    "from pyspark.sql.fuctions import col\n",
    "import pyspark.sql.fuctions as F\n",
    "\n",
    "# 새로운 컬럼 new_col 을 생성하지는 않고 schema에 Struct된 컬럼이 생성된다.\n",
    "new_df = df.withcolumn('new_col', F.from_json(col('col1', schema = schema)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. F.udf\n",
    " - DataFrame의 특정 컬럼을 사용자가 정의한 함수로 가공할 수 있게 해준다.\n",
    " - parameter = (fuction , DataType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF 명시\n",
    "# name을 input 으로 넣으면 \"/\" 기준 두번째 단어를 선택하겠다.\n",
    "udf_check_repo_name = F.udf(lambda name : name.split(\"/\")[1], StringType())\n",
    "\n",
    "# repo_name 컬럼을 생성하는데, UDF를 따라 name 컬럼에'만' 적용해서 생성하겠다)\n",
    "gitdata = gitdata.withColumn('repo_name', udf_check_repo_name(F.col('name')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. F.regexp_replace (or regexp_extract)\n",
    " - 문자열의 존재 여부 확인 혹은 모든 문자열을 치환할때 \"\"\"정규표현식\"\"\"\n",
    " - parameter = (string or dataFrame.columns , \"바뀌기 전 대상\" , \"변환후 대상\")\n",
    " - return 값은 sub string이 변환된 데이터프레임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"regexp_replace\").getOrCreate()\n",
    "\n",
    "address = [(\"RJFK-SLFKW-DFG1T\",\"M BS\", \"1990-01-13\"),\n",
    "    (\"aedw-dg93r-d62g1\",\"W SE\", \"2000-11-30\"),\n",
    "    (\"DFGE-FD23k-DA4G1\", \"M GJ\", \"1999-10-08\")]\n",
    "df =spark.createDataFrame(address,[\"id\",\"demo\",\"birth\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "#+----------------+----+----------+\n",
    "#|              id|demo|     birth|\n",
    "#+----------------+----+----------+\n",
    "#|RJFK-SLFKW-DFG1T|M BS|1990-01-13|\n",
    "#|aedw-dg93r-d62g1|W SE|2000-11-30|\n",
    "#|DFGE-FD23k-DA4G1|M GJ|1999-10-08|\n",
    "#+----------------+----+----------+\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "df = df.withColumn(\"id\", F.regexp_replace(\"id\", \"-\", \"\"))\n",
    "df.show()\n",
    "\n",
    "#+--------------+----+----------+\n",
    "#|            id|demo|     birth|\n",
    "#+--------------+----+----------+\n",
    "#|RJFKSLFKWDFG1T|M BS|1990-01-13|\n",
    "#|aedwdg93rd62g1|W SE|2000-11-30|\n",
    "#|DFGEFD23kDA4G1|M GJ|1999-10-08|\n",
    "#+--------------+----+----------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. when, split 절\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, split\n",
    "\n",
    "df = df.withColumn(\"sex\", \\\n",
    "    when(df.demo.startswith(\"M\"), regexp_replace(df.demo, \"M\", \"Man\"))\\\n",
    "    .when(df.demo.startswith(\"W\"), regexp_replace(df.demo, \"W\", \"Woman\")))\\\n",
    "    .withColumn(\"location\",\\\n",
    "    when(df.demo.endswith(\"BS\"), regexp_replace(df.demo, \"BS\", \"Busan\"))\\\n",
    "    .when(df.demo.endswith(\"SE\"), regexp_replace(df.demo, \"SE\", \"Seoul\"))\\\n",
    "    .when(df.demo.endswith(\"GJ\"), regexp_replace(df.demo, \"GJ\", \"Gwangju\"))\\\n",
    "    .otherwise(df.demo))\n",
    "\n",
    "df.show()\n",
    "\n",
    "#+----------------+----+----------+--------+---------+\n",
    "#|              id|demo|     birth|     sex| location|\n",
    "#+----------------+----+----------+--------+---------+\n",
    "#|RJFK-SLFKW-DFG1T|M BS|1990-01-13|  Man BS|  M Busan|\n",
    "#|aedw-dg93r-d62g1|W SE|2000-11-30|Woman SE|  W Seoul|\n",
    "#|DFGE-FD23k-DA4G1|M GJ|1999-11-08|  Man GJ|M Gwangju|\n",
    "#+----------------+----+----------+--------+---------+\n",
    "\n",
    "df = df.withColumn(\"sex\", split(df['sex'], ' ').getItem(0))\\\n",
    "        .withColumn(\"location\", split(df['location'], ' ').getItem(1))\n",
    "\n",
    "df.show()\n",
    "\n",
    "#+----------------+----+----------+-----+--------+\n",
    "#|              id|demo|     birth|  sex|location|\n",
    "#+----------------+----+----------+-----+--------+\n",
    "#|RJFK-SLFKW-DFG1T|M BS|1990-01-13|  Man|   Busan|\n",
    "#|aedw-dg93r-d62g1|W SE|2000-11-30|Woman|   Seoul|\n",
    "#|DFGE-FD23k-DA4G1|M GJ|1999-11-08|  Man| Gwangju|\n",
    "#+----------------+----+----------+-----+--------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. F.to_timestamp\n",
    " - parameter = (col , format'yyyy-MM-dd HH:mm:ss')\n",
    " - data type 이 date time 으로 변환\n",
    "\n",
    " # 6. F.trim\n",
    " - parameter = col or str target column to work on\n",
    " - Returns ==> trimmed values from both sides"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
